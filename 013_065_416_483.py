# -*- coding: utf-8 -*-
"""013_065_416_483.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4sMKnIGRDlKWBcjePZZL2YwKaXvfUQ4

1.PES1UG19CS065 Anirudh BS
2.PES1UG19CS416 Sahishnu
3.PES1UG19CS013 Abhijith S
4.PES1UG19CS483 Siddharth
"""

import numpy as np
import random
import networkx as nx
from IPython.display import Image
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

edges=pd.read_csv('/content/sample_data/got-s2-edges.csv')

G = nx.from_pandas_edgelist(edges, 'Source', 'Target', 'Weight')

"""Loading Graph and EDA"""

#Plotting the graph

plt.figure(3, figsize=(20,20))
pos = nx.fruchterman_reingold_layout(G)

nx.draw_networkx(G, pos=pos, node_size = 400, font_size=4, node_color="lightgreen")
nx.draw_networkx_edges(G, pos=pos)
edge_labels = nx.get_edge_attributes(G, 'Weight')
nx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels)
plt.show()

n = G.number_of_nodes()
m = G.number_of_edges()
print("Number of nodes =", str(n))
print("Number of edges =", str(m))
print("Number of connected components =", str(nx.number_connected_components(G)))

#checking for isolates
list(nx.isolates(G))

#checking for self loops
list(nx.selfloop_edges(G))

"""Calculating and setting Node features"""

from operator import itemgetter

#Betweenness centrality
c_betweenness = nx.betweenness_centrality(G)
sorted(c_betweenness.items(), key=itemgetter(1), reverse=True)

#Pagerank 

s='\n sorted pagerank:\n'
print (s)
pgr=nx.pagerank(G)
print (sorted(pgr.items(), key=itemgetter(1), reverse=True))

#Local Clustering Coefficient

s='\nLocal Clustering Coefficient :'
print (s)
localclust=nx.clustering(G)
print(localclust)

#Setting Node attributes

nx.set_node_attributes(G, c_betweenness, "betweenness")
nx.set_node_attributes(G, pgr, "pagerank")
nx.set_node_attributes(G, nx.clustering(G), "localclustering")

"""Spectral clustering to find Communities"""

# Class to perform spectral clustering for k clusters, given k

class SpectralClustering:
    
    def __init__(self, G):
        # Initialize the graph, nodes and the normalized Laplacian

        self.graph = G
        self.normalized_laplacian = nx.linalg.normalized_laplacian_matrix(G).toarray()
        self.nodes = G.nodes()

    def compute_eigenspace(self):
        # Compute the eigenvalues and the eigenvectors using Numpy and sort them in ascending order 

        eigenvalues, eigenvectors = np.linalg.eig(self.normalized_laplacian)

        sort_index = np.argsort(eigenvalues) # Returns the list of indices that would sort the array
        sorted_eigenvectors = eigenvectors[:, sort_index]

        return sorted_eigenvectors

    def fit(self, k):
        """
        Step 1: Extract the k eigen vectors and stack them vertically to form a matrix (i.e each vector is a column)
        Step 2: Every row of this matrix represents the features of the respective nodes of the graph
        Step 3: Perform K-Means clustering on this dataset to identify k clusters

        """
        eigenvectors = self.compute_eigenspace()
        k_eigenvectors = np.array(eigenvectors)[:,:k]
        dataset = pd.DataFrame(k_eigenvectors, index=self.nodes)

        # print(dataset)

        k_means_clusters = KMeans(n_clusters = k, random_state=42)
        self.clusters = k_means_clusters.fit(k_eigenvectors)
        
    def predict_labels(self):
        # Returns the labels 
        return self.clusters.labels_

    def assign_labels(self, labels):
        # Helper to assign the labels as node attributes
        for i,node in enumerate(self.graph.nodes()):
            self.graph.nodes[node]['label'] = labels[i]

    def plot_graph(self, n_clusters, title='Graph after Spectral Clustering'):
        # Plots the graph 

        labels = self.clusters.labels_
        self.assign_labels(labels)

        colour_list = ['blue','green','red','yellow','cyan', 'magenta','lightblue','grey']
        sampled_colours = dict(zip(set(labels),random.sample(colour_list, n_clusters)))

        legend_handles = []
        for label, color in sampled_colours.items():
            colour_handle = mpatches.Patch(color=color, label=label)
            legend_handles.append(colour_handle)

        colours = [sampled_colours[i] for i in labels]

        pos_fr = nx.fruchterman_reingold_layout(self.graph)
        plt.figure(figsize=(50,50))
        plt.title(title)
        plt.legend(handles=legend_handles)
        nx.draw(self.graph, pos=pos_fr, node_size=400, node_color=colours, with_labels=True)
        plt.show()

from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
from itertools import count 
import matplotlib.patches as mpatches

# Performing Spectral Clustering on the graph
k = 8
SpecClust = SpectralClustering(G)
SpecClust.fit(k)
clustering_labels = SpecClust.predict_labels()
# print(labels)

for i,node in enumerate(G.nodes()):
    G.nodes[node]['label'] = clustering_labels[i]

colour_map = ['']*len(clustering_labels)
for i, label in enumerate(clustering_labels):
    if label == 0:
        colour_map[i] = 'lightgreen'
    else:
        colour_map[i] = "red"

# nx.draw(G, with_labels=True, node_color=colour_map)
SpecClust.plot_graph(k, title="Graph after Spectral Clustering")

"""Why the communities detected are highly likely communities in the story?"""

#Hypothesis on the communities detected

"""Communities are formed based on associations between the nodes and/or where they happen to be in the second season. Below, is a detailed overview for each of the detected communities.

Pink Community -> Ilyn Payne, Bronn, Shae, Tyrion, Tommen, Cersei, Joffrey, Varys, Littlefinger, Hound, John Arrys, Tywin, Aerys, Syrio Forrel, High Septon, Myrcella
These characters were mostly in King's Landing and belong to the Royal family or are close to the members of the royal family

Neon Blue Community -> Daenarys, Drogo, Rhaego, Malakko, Rakharo, Kovarro, Jorah, Spice king, Silk king, Pyatt Pree, Xaro
These characters mostly belong to the Dothraki group or are associated to Daenerys, who is the queen of the Dothrakis. Spice king, Silk king, Pyatt Pree, Xaro are from Qarth, where Daenerys goes to with her dragons after Drogo dies

Red Community -> Stannis, Davos, Marya, Melisandre, Brienne, Renly, Margaery, Ramsay, Roose Bolton, Frey's Daughter, Robb, Telisa, Catelyn, Gerard, Torrhen Karstark, Rickard Karstark.

Stannis, Davos, Marya, Melisandre are associated Stannis Baratheon.
Brienne, Renly, Margaery are associated Renly Baratheon
Ramsay, Roose Bolton, Frey's Daughter, Robb, Telisa, Catelyn, Gerard, Torrhen Karstark, Rickard Karstark are associated with Robb
The characters Stannis, Renly and Robb share a common interest, i.e they all are aiming to defeat Joffrey who has assumed the throne unlawfully.

Gray Community -> Jaqen, Arya, Gendry, Hot Pie, Mountain, Amory
These characters are either the prisoners or the captors(working for Lannisters)

Light Blue Community -> Benjen, Gilly, Sam, Melessa, Randyll, Eddison Tollett, Jeor, Raster, Grenn
Most of these characters are part of the Night's Watch. The characters Gilly,Melessa and Randyll are related to Sam who belongs to the Night's watch.

Dark Blue Community -> Jon, Ygritte, Lord of Bones, Mance, Qhorin
These characters either resided behind the wall (free-folk), or they (Jon and Qhorin) had gone there to negotiate.

Green Community -> Hodor, Rickon, Maester Luwin, Winterfell Shepherd, Bran, Osha, Theon, Yara, Dagmer, Black Lorren
These characters mostly reside in Winterfell. Some characters, like Theon, Yara, Dagmer, Black Lorren were part of the group that took over Winterfell.

Yellow Community -> Mhaegan, Ros, Barra, Janos
Ros and Mhaegan work in a brothel and Barra is Mhaegan's son (bastard of King Robert).
"""



"""Link prediction using Adamic Adar, Jaccard and Preferential Attachment and calculating their Accuracies"""

# take a random sample of edges

proportion_edges = 0.25
edge_subset = random.sample(G.edges(), int(proportion_edges * G.number_of_edges()))

G_train = G.copy()
G_train.remove_edges_from(edge_subset)

plt.figure(figsize=(20, 20))
nx.draw(G_train, with_labels = True)

edge_subset_size = len(list(edge_subset))
print("Deleted edges =", str(edge_subset_size))
print("Remaining edges =", str(m - edge_subset_size))

print("Deleted edges : ", edge_subset)



prediction_jaccard = list(nx.jaccard_coefficient(G_train))
score_jaccard, label_jaccard = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in prediction_jaccard])

# binary classification here. taking various thresholds for the jaccard coefficient.
# calculating fpr, tpr, and auc values for various thresholds of the jaccard coefficient.

fpr_jaccard, tpr_jaccard, thresholds_jaccard = roc_curve(label_jaccard, score_jaccard)
auc_jaccard = roc_auc_score(label_jaccard, score_jaccard)



prediction_adamic = list(nx.adamic_adar_index(G_train))
score_adamic, label_adamic = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in prediction_adamic])

fpr_adamic, tpr_adamic, thresholds_adamic = roc_curve(label_adamic, score_adamic)
auc_adamic = roc_auc_score(label_adamic, score_adamic)



prediction_pref = list(nx.preferential_attachment(G_train))
score_pref, label_pref = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in prediction_pref])

fpr_pref, tpr_pref, thresholds_pref = roc_curve(label_pref, score_pref)
auc_pref = roc_auc_score(label_pref, score_pref)

plt.figure(figsize=(12, 8))
plt.plot(fpr_jaccard, tpr_jaccard, label='Jaccard coefficient - AUC %.2f' % auc_jaccard, linewidth=4)
plt.plot(fpr_adamic, tpr_adamic, label='Adamic-Adar - AUC %.2f' % auc_adamic, linewidth=4)
plt.plot(fpr_pref, tpr_pref, label='Preferential attachment - AUC %.2f' % auc_pref, linewidth=4)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title("ROC AUC Curve")
plt.legend(loc='lower right')
plt.show()

#Adamic Adar shows better performance -> higher AUC

#Adamic Adar
optimal_idx = np.argmax(tpr_adamic - fpr_adamic)
optimal_threshold = thresholds_adamic[optimal_idx]
print("Optimal threshold =", optimal_threshold)

#Jaccard
optimal_idx = np.argmax(tpr_jaccard - fpr_jaccard)
optimal_threshold = thresholds_jaccard[optimal_idx]
print("Optimal threshold =", optimal_threshold)

#Pref Attachment
optimal_idx = np.argmax(tpr_pref - fpr_pref)
optimal_threshold = thresholds_pref[optimal_idx]
print("Optimal threshold =", optimal_threshold)

predicted_edges_adamic=[]
predicted_edges_jaccard=[]
predicted_edges_pref=[]

for (u,v,p) in prediction_adamic:
  if(p>0.369):
    predicted_edges_adamic.append((u,v))

for (u,v,p) in prediction_jaccard:
  if(p>0.058):
    predicted_edges_jaccard.append((u,v))

for (u,v,p) in prediction_pref:
  if(p>69):
    predicted_edges_pref.append((u,v))

count_adamic=0
for i in range(len(edge_subset)):
  for j in range(len(predicted_edges_adamic)):
    if((edge_subset[i][0]==predicted_edges_adamic[j][0] and edge_subset[i][1]==predicted_edges_adamic[j][1]) or (edge_subset[i][0]==predicted_edges_adamic[j][1] and edge_subset[i][1]==predicted_edges_adamic[j][0])):
      count_adamic+=1

count_jaccard=0
for i in range(len(edge_subset)):
  for j in range(len(predicted_edges_jaccard)):
    if((edge_subset[i][0]==predicted_edges_jaccard[j][0] and edge_subset[i][1]==predicted_edges_jaccard[j][1]) or (edge_subset[i][0]==predicted_edges_jaccard[j][1] and edge_subset[i][1]==predicted_edges_jaccard[j][0])):
      count_jaccard+=1

count_pref=0
for i in range(len(edge_subset)):
  for j in range(len(predicted_edges_pref)):
    if((edge_subset[i][0]==predicted_edges_pref[j][0] and edge_subset[i][1]==predicted_edges_pref[j][1]) or (edge_subset[i][0]==predicted_edges_pref[j][1] and edge_subset[i][1]==predicted_edges_pref[j][0])):
      count_pref+=1

print("Adamic acc = ",count_adamic/len(edge_subset))
print("Jaccard acc = ",count_jaccard/len(edge_subset))
print("PA acc = ",count_pref/len(edge_subset))

print("Adamic correct predictions = ", count_adamic/len(edge_subset))

# 88% of deleted edges predicted using link prediction with adamic adar

"""Link Prediction using GraphML"""

#Install the necessary packages
!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html
!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html
!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git
!pip install --upgrade torch-cluster

!pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html

import torch
import torch_geometric

from torch_geometric.utils.convert import from_networkx

pyg_graph = from_networkx(G)

print(pyg_graph)

#SAGE

# Commented out IPython magic to ensure Python compatibility.
import sys
if 'google.colab' in sys.modules:
#   %pip install -q stellargraph[demos]==1.2.1

import stellargraph as sg

try:
    sg.utils.validate_notebook_version("1.2.1")
except AttributeError:
    raise ValueError(
        f"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>."
    ) from None

# Commented out IPython magic to ensure Python compatibility.
import stellargraph as sg
from stellargraph.data import EdgeSplitter
from stellargraph.mapper import GraphSAGELinkGenerator
from stellargraph.layer import GraphSAGE, HinSAGE, link_classification

from tensorflow import keras
from sklearn import preprocessing, feature_extraction, model_selection

from stellargraph import globalvar
from stellargraph import datasets
from IPython.display import display, HTML
# %matplotlib inline

from stellargraph import StellarGraph

#listing node attributes
G.nodes["ARYA"]

for node_id in G:
  print(G.nodes[node_id])

Gs = StellarGraph.from_networkx(G, node_features=G.nodes())

Gs.info()

# Define an edge splitter on the original graph G:
edge_splitter_test = EdgeSplitter(Gs)

# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G, and obtain the
# reduced graph G_test with the sampled links removed:
G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(
    p=0.1, method="global", keep_connected=True
)

# Define an edge splitter on the reduced graph G_test:
edge_splitter_train = EdgeSplitter(G_test)

# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the
# reduced graph G_train with the sampled links removed:
G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(
    p=0.1, method="global", keep_connected=True
)

print(G_train.info())

print(G_test.info()) #same nodes but different edge sets than G_train

batch_size = 10
epochs = 100
num_samples = [20, 10, 10]

train_gen = GraphSAGELinkGenerator(G_train, batch_size, num_samples)
train_flow = train_gen.flow(edge_ids_train, edge_labels_train, shuffle=True)

test_gen = GraphSAGELinkGenerator(G_test, batch_size, num_samples)
test_flow = test_gen.flow(edge_ids_test, edge_labels_test)

layer_sizes = [20, 20, 20]
graphsage = GraphSAGE(
    layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3
)

# Build the model and expose input and output sockets of graphsage model
# for link prediction
x_inp, x_out = graphsage.in_out_tensors()

prediction = link_classification(
    output_dim=1, output_act="relu", edge_embedding_method="ip"
)(x_out)

model = keras.Model(inputs=x_inp, outputs=prediction)

model.compile(
    optimizer=keras.optimizers.Adam(lr=1e-3),
    loss=keras.losses.binary_crossentropy,
    metrics=["acc"],
)

#Evaluate the initial (untrained) model on the train and test set:

init_train_metrics = model.evaluate(train_flow)
init_test_metrics = model.evaluate(test_flow)

print("\nTrain Set Metrics of the initial (untrained) model:")
for name, val in zip(model.metrics_names, init_train_metrics):
    print("\t{}: {:0.4f}".format(name, val))

print("\nTest Set Metrics of the initial (untrained) model:")
for name, val in zip(model.metrics_names, init_test_metrics):
    print("\t{}: {:0.4f}".format(name, val))

history = model.fit(train_flow, epochs=epochs, validation_data=test_flow, verbose=2)

sg.utils.plot_history(history)

#Evaluate the trained model on test links:

train_metrics = model.evaluate(train_flow)
test_metrics = model.evaluate(test_flow)

print("\nTrain Set Metrics of the trained model:")
for name, val in zip(model.metrics_names, train_metrics):
    print("\t{}: {:0.4f}".format(name, val))

print("\nTest Set Metrics of the trained model:")
for name, val in zip(model.metrics_names, test_metrics):
    print("\t{}: {:0.4f}".format(name, val))

"""The accuracy of the Sage Model is very poor when compared to the accuracies obtained from traditional models"""

#GCN

graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)
graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset
graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)

# Define an edge splitter on the original graph G:
edge_splitter_test = EdgeSplitter(Gs)

# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G, and obtain the
# reduced graph G_test with the sampled links removed:
G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(
    p=0.1, method="global", keep_connected=True
)

# Define an edge splitter on the reduced graph G_test:
edge_splitter_train = EdgeSplitter(G_test)

# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the
# reduced graph G_train with the sampled links removed:
G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(
    p=0.1, method="global", keep_connected=True
)

epochs = 100

train_gen = FullBatchLinkGenerator(G_train, method="gcn")
train_flow = train_gen.flow(edge_ids_train, edge_labels_train)

test_gen = FullBatchLinkGenerator(G_test, method="gcn")
test_flow = train_gen.flow(edge_ids_test, edge_labels_test)

gcn = GCN(
    layer_sizes=[16, 16], activations=["relu", "relu"], generator=train_gen, dropout=0.3
)

x_inp, x_out = gcn.in_out_tensors()

prediction = LinkEmbedding(activation="relu", method="ip")(x_out)

prediction = keras.layers.Reshape((-1,))(prediction)

model = keras.Model(inputs=x_inp, outputs=prediction)

model.compile(
    optimizer=keras.optimizers.Adam(lr=0.01),
    loss=keras.losses.binary_crossentropy,
    metrics=["acc"],
)

init_train_metrics = model.evaluate(train_flow)
init_test_metrics = model.evaluate(test_flow)

print("\nTrain Set Metrics of the initial (untrained) model:")
for name, val in zip(model.metrics_names, init_train_metrics):
    print("\t{}: {:0.4f}".format(name, val))

print("\nTest Set Metrics of the initial (untrained) model:")
for name, val in zip(model.metrics_names, init_test_metrics):
    print("\t{}: {:0.4f}".format(name, val))

"""Compared to the GraphML models, the traditional models has better performance in terms of the Accuracy. Among the traditional models, Adamic Adar performed the best."""



